{
  "name": "multi-task-etl-pipeline",
  "email_notifications": {
    "on_failure": ["data-team@company.com"],
    "on_success": ["data-team@company.com"],
    "no_alert_for_skipped_runs": false
  },
  "webhook_notifications": {},
  "timeout_seconds": 0,
  "schedule": {
    "quartz_cron_expression": "0 0 2 * * ?",
    "timezone_id": "UTC",
    "pause_status": "UNPAUSED"
  },
  "max_concurrent_runs": 1,
  "tasks": [
    {
      "task_key": "bronze_ingestion",
      "description": "Ingest raw data using Auto Loader",
      "notebook_task": {
        "notebook_path": "/Repos/org/databricks_cheat_sheets/etl/autoloader",
        "base_parameters": {
          "run_date": "{{job.start_time.iso_date}}",
          "source_path": "/mnt/raw/events/",
          "target_table": "main.bronze.events"
        }
      },
      "job_cluster_key": "bronze_cluster",
      "timeout_seconds": 3600,
      "email_notifications": {},
      "notification_settings": {
        "no_alert_for_skipped_runs": false,
        "no_alert_for_canceled_runs": false,
        "alert_on_last_attempt": false
      }
    },
    {
      "task_key": "silver_transformation",
      "description": "Transform bronze to silver with data quality checks",
      "depends_on": [
        {
          "task_key": "bronze_ingestion"
        }
      ],
      "notebook_task": {
        "notebook_path": "/Repos/org/databricks_cheat_sheets/pyspark_code/data_cleaning",
        "base_parameters": {
          "source_table": "main.bronze.events",
          "target_table": "main.silver.events_cleaned",
          "run_date": "{{job.start_time.iso_date}}"
        }
      },
      "job_cluster_key": "silver_cluster",
      "timeout_seconds": 7200
    },
    {
      "task_key": "gold_aggregation",
      "description": "Create gold layer aggregations",
      "depends_on": [
        {
          "task_key": "silver_transformation"
        }
      ],
      "sql_task": {
        "query": {
          "query_id": "12345-abcd-6789-efgh-123456789012"
        },
        "warehouse_id": "your-warehouse-id"
      },
      "timeout_seconds": 3600
    },
    {
      "task_key": "data_quality_checks",
      "description": "Validate data quality and send alerts",
      "depends_on": [
        {
          "task_key": "gold_aggregation"
        }
      ],
      "python_wheel_task": {
        "package_name": "data_validation",
        "entry_point": "validate_pipeline",
        "parameters": [
          "--table", "main.gold.daily_metrics",
          "--date", "{{job.start_time.iso_date}}",
          "--alert-channel", "#data-alerts"
        ]
      },
      "job_cluster_key": "validation_cluster",
      "timeout_seconds": 1800
    },
    {
      "task_key": "table_maintenance",
      "description": "Optimize and vacuum tables",
      "depends_on": [
        {
          "task_key": "data_quality_checks"
        }
      ],
      "sql_task": {
        "query": {
          "query": "-- Optimize key tables\nOPTIMIZE main.silver.events_cleaned ZORDER BY (event_date, user_id);\nOPTIMIZE main.gold.daily_metrics ZORDER BY (date, country);\n\n-- Vacuum old files (keep 7 days)\nVACUUM main.silver.events_cleaned RETAIN 168 HOURS;\nVACUUM main.gold.daily_metrics RETAIN 168 HOURS;"
        },
        "warehouse_id": "your-warehouse-id"
      },
      "timeout_seconds": 1800
    },
    {
      "task_key": "notification_task",
      "description": "Send success notification with pipeline metrics",
      "depends_on": [
        {
          "task_key": "table_maintenance"
        }
      ],
      "notebook_task": {
        "notebook_path": "/Repos/org/databricks_cheat_sheets/admin/pipeline_notifications",
        "base_parameters": {
          "bronze_count": "{{tasks.bronze_ingestion.values.record_count}}",
          "silver_count": "{{tasks.silver_transformation.values.processed_count}}",
          "pipeline_status": "SUCCESS",
          "run_id": "{{job.run_id}}"
        }
      },
      "job_cluster_key": "notification_cluster",
      "timeout_seconds": 600
    }
  ],
  "job_clusters": [
    {
      "job_cluster_key": "bronze_cluster",
      "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.xlarge",
        "driver_node_type_id": "i3.xlarge",
        "num_workers": 2,
        "autoscale": {
          "min_workers": 1,
          "max_workers": 8
        },
        "aws_attributes": {
          "availability": "SPOT_WITH_FALLBACK",
          "zone_id": "auto",
          "spot_bid_price_percent": 100,
          "ebs_volume_count": 1,
          "ebs_volume_size": 32,
          "ebs_volume_type": "gp3"
        },
        "spark_conf": {
          "spark.databricks.delta.autoCompact.enabled": "true",
          "spark.databricks.delta.optimizeWrite.enabled": "true",
          "spark.sql.adaptive.enabled": "true"
        },
        "custom_tags": {
          "Environment": "production",
          "Team": "data-engineering",
          "Purpose": "etl-pipeline"
        }
      }
    },
    {
      "job_cluster_key": "silver_cluster",
      "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "r5.2xlarge",
        "driver_node_type_id": "r5.2xlarge",
        "num_workers": 4,
        "autoscale": {
          "min_workers": 2,
          "max_workers": 12
        },
        "aws_attributes": {
          "availability": "SPOT_WITH_FALLBACK",
          "zone_id": "auto",
          "spot_bid_price_percent": 100
        },
        "spark_conf": {
          "spark.sql.adaptive.enabled": "true",
          "spark.sql.adaptive.coalescePartitions.enabled": "true",
          "spark.sql.adaptive.skewJoin.enabled": "true"
        }
      }
    },
    {
      "job_cluster_key": "validation_cluster",
      "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.large",
        "driver_node_type_id": "i3.large",
        "num_workers": 0,
        "custom_tags": {
          "Environment": "production",
          "Purpose": "validation"
        }
      }
    },
    {
      "job_cluster_key": "notification_cluster",
      "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.large",
        "driver_node_type_id": "i3.large",
        "num_workers": 0
      }
    }
  ],
  "git_source": {
    "git_url": "https://github.com/your-org/databricks_cheat_sheets",
    "git_provider": "gitHub",
    "git_branch": "main"
  },
  "format": "MULTI_TASK",
  "queue": {
    "enabled": false
  },
  "parameters": [
    {
      "name": "environment",
      "default": "production"
    },
    {
      "name": "run_date",
      "default": "{{job.start_time.iso_date}}"
    }
  ]
}