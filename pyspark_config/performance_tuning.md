# ⚙️ Spark/Delta Performance Tuning Cheatsheet

- File sizes
  - Aim for 128–512 MB data files. Use OPTIMIZE for Delta to bin-pack small files.
  - Avoid excessive small files from too many partitions or tiny batches.
- Partitioning
  - Partition on low-cardinality, high-selectivity columns (e.g., date, country).
  - Avoid over-partitioning; keep <5k files per partition for quick listing.
- Shuffle tuning
  - Enable AQE: `spark.sql.adaptive.enabled=true`.
  - Let AQE coalesce partitions; set a reasonable default: `spark.sql.shuffle.partitions=auto` (DBR 13+).
  - Broadcast small tables: `/*+ BROADCAST(dim) */` or `spark.sql.autoBroadcastJoinThreshold`.
- Skew handling
  - Use AQE skew join handling: `spark.sql.adaptive.skewJoin.enabled=true`.
  - Salt keys for extreme skew; or pre-aggregate on the skewed side.
- Caching and checkpointing
  - Cache only reused DataFrames; unpersist when done.
  - For streaming state stores, checkpoint to reliable storage and set watermarks.
- Delta table health
  - `OPTIMIZE <table> ZORDER BY (hot_columns)`.
  - `VACUUM <table> RETAIN 168 HOURS` (weekly cleanup).
  - Set table props: autoOptimize, log/deleted retention.
- Query patterns
  - Push filters early; select only needed columns.
  - Prefer window aggregates over self-joins when possible.
- Concurrency
  - Use `IsolationLevel.SnapshotIsolation` (default) and idempotent MERGE patterns.
- Monitoring
  - Track `DESCRIBE DETAIL/HISTORY`, the Spark UI, and Delta metrics in logs.
